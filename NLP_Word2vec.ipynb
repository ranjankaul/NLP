{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "lU20fzbOjHaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a2a341e-2b9f-4ebb-ff48-0f32e3b14eb5"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GfogMC2IjtdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e8514cd0-4428-4b84-841d-e0c58f74fb8a"
      },
      "cell_type": "code",
      "source": [
        "!ls /content/drive/'My Drive'/NLP_vec/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "got1.txt  got2.txt  got3.txt  got4.txt\tgot5.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKnPko-3kn2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import codecs                        ## word encoding\n",
        "import glob                          ## regex\n",
        "import multiprocessing               ## concurrency\n",
        "import pprint                        ## pretty printing\n",
        "import nltk\n",
        "import gensim.models.word2vec as w2v ## word2 vec model\n",
        "import sklearn.manifold              ## dimensonality reduction\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zSLIAYZhkqdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1f1a75e-5526-4281-b6d5-660eb183b78b"
      },
      "cell_type": "code",
      "source": [
        "## Load books from files::\n",
        "directory = \"/content/drive/My Drive/NLP_vec/\"\n",
        "os.chdir(directory)\n",
        "\n",
        "book_list = sorted(os.listdir())\n",
        "print(book_list)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['got1.txt', 'got2.txt', 'got3.txt', 'got4.txt', 'got5.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uhY0McetmAzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "5e596d01-cd59-4110-ad88-c24ffca5e0ab"
      },
      "cell_type": "code",
      "source": [
        "## Combine the books into one string\n",
        "corpus_raw = u\"\"\n",
        "for book in book_list:\n",
        "    print(\"Reading '{0}'...\".format(book))\n",
        "    with codecs.open(book, \"r\", \"utf-8\") as book_file:\n",
        "        corpus_raw += book_file.read()\n",
        "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading 'got1.txt'...\n",
            "Corpus is now 1768233 characters long\n",
            "Reading 'got2.txt'...\n",
            "Corpus is now 3526269 characters long\n",
            "Reading 'got3.txt'...\n",
            "Corpus is now 5808346 characters long\n",
            "Reading 'got4.txt'...\n",
            "Corpus is now 7469273 characters long\n",
            "Reading 'got5.txt'...\n",
            "Corpus is now 9774033 characters long\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6k4YkZk9oYqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## NLTK : Tokenizer\n",
        "\n",
        "tokenizer  = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "raw_sentences = tokenizer.tokenize(corpus_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PGWXOklaoe56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert into a list of words remove unnecessary,, split into words, no hyphens just list of words\n",
        "def sentence_to_wordlist(raw):\n",
        "    clean = re.sub(\"[^a-zA-Z]\" ,\" \", raw)\n",
        "    words = clean.split()\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4t7znutnpCIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "66f0ba11-8bc0-45b6-a56c-02fe11c7c4eb"
      },
      "cell_type": "code",
      "source": [
        "# sentence where each word is tokenized\n",
        "sentences = []\n",
        "for raw_sentence in raw_sentences:\n",
        "    if len(raw_sentence) > 0:\n",
        "        sentences.append(sentence_to_wordlist(raw_sentence))\n",
        "\n",
        "print(raw_sentences[5])\n",
        "print(sentence_to_wordlist(raw_sentences[5]))\n",
        "token_count = (sum([len(sentence) for sentence in sentences]))\n",
        "\n",
        "print(\"The corpus has {0:,} tokens\".format(token_count))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now he thought of them as old friends.\n",
            "['Now', 'he', 'thought', 'of', 'them', 'as', 'old', 'friends']\n",
            "The corpus has 1,836,474 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tCanRh5HpHrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "507426ce-b048-4b35-aed3-c6468b745cd4"
      },
      "cell_type": "code",
      "source": [
        "######################################################### Train Word2Vec ###############################################\n",
        "\n",
        "''' ONCE we have vectors step 3 - build model : \n",
        "\n",
        "    3 main tasks that vectors help with\n",
        "    DISTANCE, SIMILARITY, RANKING\n",
        "\n",
        "    Dimensionality of the resulting word vectors.\n",
        "    more dimensions, more computationally expensive to train\n",
        "    but also more accurate\n",
        "    more dimensions = more generalized\n",
        "'''\n",
        "num_features = 300  # ''' Dimension of the resulting word vector'''\n",
        "min_word_count = 3  # ''' Minimum word count threshold. '''\n",
        "num_workers = multiprocessing.cpu_count()  # ''' Number of threads to run in parallel. : more workers, faster we train '''\n",
        "context_size = 7  # ''' Context window length. '''\n",
        "downsampling = 1e-3  # ''' Downsample setting for frequent words. 0 - 1e-5 is good for this '''\n",
        "seed = 1  # ''' Seed for the RNG, to make the results reproducible. random number generator,deterministic, good for debugging '''\n",
        "\n",
        "got2vec = w2v.Word2Vec(sg=1,\n",
        "                       seed=seed,\n",
        "                       workers=num_workers,\n",
        "                       size=num_features,\n",
        "                       min_count=min_word_count,\n",
        "                       window=context_size,\n",
        "                       sample=downsampling)\n",
        "\n",
        "got2vec.build_vocab(sentences)\n",
        "print(\"Word2Vec vocabulary length:\", len(got2vec.wv.vocab))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec vocabulary length: 17301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QWCtVGjcpMWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a6c9d8f4-b017-4bdd-ba3b-aa2f75e846a7"
      },
      "cell_type": "code",
      "source": [
        "got2vec.train(sentences ,total_examples = got2vec.corpus_count ,epochs = got2vec.iter)\n",
        "\n",
        "''' Create directory '''\n",
        "\n",
        "if not os.path.isdir('trained_model'):\n",
        "    print('new directory has been created')\n",
        "\n",
        "  \n",
        "got2vec.save(\"got2vec.w2v\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new directory has been created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q9B2FN5SqgiL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "66769ee4-5181-48cd-ef14-9f0fa29370d9"
      },
      "cell_type": "code",
      "source": [
        "got2vec = w2v.Word2Vec.load(\"got2vec.w2v\")\n",
        "\n",
        "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\n",
        "\n",
        "all_word_vectors_matrix = got2vec.wv.syn0\n",
        "\n",
        "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)\n",
        "\n",
        "points = pd.DataFrame(\n",
        "    [\n",
        "        (word, coords[0], coords[1])\n",
        "        for word, coords in [\n",
        "            (word, all_word_vectors_matrix_2d[got2vec.wv.vocab[word].index])\n",
        "            for word in got2vec.wv.vocab\n",
        "       ]\n",
        "    ],\n",
        "    columns=[\"word\", \"x\", \"y\"]\n",
        ")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-n1U8kX7qibJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "ecdd3a07-e4e0-4d23-b0ae-48b36aaef161"
      },
      "cell_type": "code",
      "source": [
        "got2vec.most_similar_cosmul(\"blood\")     "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('veins', 0.7431859970092773),\n",
              " ('brains', 0.7431262135505676),\n",
              " ('smeared', 0.7329888343811035),\n",
              " ('Blood', 0.7278515100479126),\n",
              " ('sap', 0.7251682281494141),\n",
              " ('trickling', 0.7224265933036804),\n",
              " ('valonqar', 0.7211002111434937),\n",
              " ('venom', 0.7201265692710876),\n",
              " ('salty', 0.7200968861579895),\n",
              " ('excrement', 0.7197314500808716)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}